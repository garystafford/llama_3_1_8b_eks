# Model Downloader ServiceAccount and ConfigMap
#
# This creates the Kubernetes resources needed for S3-based model loading:
# - ServiceAccount with IRSA annotation for AWS S3 access
# - ConfigMap with the download script used by init containers
#
# Prerequisites:
#   1. Apply the Terraform in terraform/ to create the IAM role
#   2. Get the role ARN: terraform output model_downloader_role_arn
#   3. Update the annotation below with the actual role ARN
#
# Usage:
#   kubectl apply -f model-downloader-sa.yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: model-downloader
  namespace: analysis
  annotations:
    # Replaced by Kustomize from config
    eks.amazonaws.com/role-arn: arn:aws:iam::$(AWS_ACCOUNT_ID):role/$(IAM_ROLE_NAME)
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: model-download-script
  namespace: analysis
data:
  download-llm.sh: |
    #!/bin/bash
    set -e

    # Configuration from environment variables
    DOWNLOAD_DIR="/models/.cache/huggingface/hub"
    MARKER_FILE="/models/.cache/.llm-download-complete"
    S3_BUCKET="s3://${AWS_ACCOUNT_ID}-sagemaker-${AWS_REGION}/${S3_BUCKET_NAME}"
    MODEL_DIR="models--${MODEL_REPO_DASHED}"

    mkdir -p "${DOWNLOAD_DIR}/${MODEL_DIR}"

    # Check if already downloaded
    if [ -f "${MARKER_FILE}" ]; then
      echo "LLM models already cached, skipping download"
      ls -lh "${DOWNLOAD_DIR}/${MODEL_DIR}/" 2>/dev/null | head -5 || true
      exit 0
    fi

    echo "Downloading LLM models from S3..."
    echo "Source: ${S3_BUCKET}/llm/"
    echo "Destination: ${DOWNLOAD_DIR}/${MODEL_DIR}/"
    echo "Model: ${MODEL_NAME}"

    # Download LLM model files
    aws s3 sync "${S3_BUCKET}/llm/" "${DOWNLOAD_DIR}/${MODEL_DIR}/" \
      --exclude ".cache/*" \
      --no-progress \
      --only-show-errors

    # Verify download
    if [ -f "${DOWNLOAD_DIR}/${MODEL_DIR}/config.json" ]; then
      echo "LLM model download complete"
      touch "${MARKER_FILE}"
      ls -lh "${DOWNLOAD_DIR}/${MODEL_DIR}/" 2>/dev/null | head -10 || true
    else
      echo "ERROR: LLM model download failed - config.json not found"
      exit 1
    fi
